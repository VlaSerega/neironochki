{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515876e8",
   "metadata": {},
   "source": [
    "# Двухслойная нейронка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612ed94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05c5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return np.exp(-x) / ((1 + np.exp(-x)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c5315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.zeros((output_size, 1))\n",
    "        self.input = None\n",
    "        self.gradient_weights = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.input = input_data\n",
    "        return np.dot(self.weights, input_data) + self.bias\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        self.gradient_weights = np.dot(gradient, self.input.T)\n",
    "        self.gradient_bias = np.sum(gradient, axis=1, keepdims=True)\n",
    "        return np.dot(self.weights.T, gradient)\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.weights -= learning_rate * self.gradient_weights\n",
    "        self.bias -= learning_rate * self.gradient_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373b4eca-cbdd-4ccf-aa0b-964496842606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, data):\n",
    "        self.out = self.sigmoid(data)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, data):\n",
    "        return data * self.out * (1 - self.out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622ba033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers, epochs, learn_rate, is_reg=False, verbose=True):\n",
    "        self.lyers = layers\n",
    "        self.epochs = epochs\n",
    "        self.lr = learn_rate\n",
    "        self.layers = layers\n",
    "        self.is_reg = is_reg\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def forward(self, x_train):\n",
    "        output = x_train\n",
    "        \n",
    "        for l in self.layers:\n",
    "            output = l.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, y_train, output):\n",
    "        error = 2 * (output - y_train)\n",
    "        \n",
    "        i = len(self.layers) - 1\n",
    "        for l in reversed(self.layers):\n",
    "            error = l.backward(error)\n",
    "    \n",
    "    def update(self):\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, Sigmoid):\n",
    "                continue\n",
    "            l.update(self.lr)\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        prediction = []\n",
    "        \n",
    "        for x in x_test:\n",
    "            out = self.forward(x)\n",
    "            if self.is_reg:\n",
    "                prediction.append(out[0, 0])\n",
    "            else:\n",
    "                prediction.append(np.argmax(out))\n",
    "        \n",
    "        return np.array(prediction)\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        for i in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for j, x in enumerate(x_train):\n",
    "                if not self.is_reg:\n",
    "                    targets = np.zeros(10).reshape(-1, 1)\n",
    "                    targets[y_train[j]] = 1\n",
    "                else:\n",
    "                    targets = np.array([0]).reshape(-1, 1).astype('float64')\n",
    "                    targets[0] = y_train[j]\n",
    "                out = self.forward(x.reshape(-1, 1))\n",
    "                total_loss += np.mean((out - targets) ** 2)\n",
    "                self.backward(targets, out)\n",
    "                self.update()\n",
    "            if (i + 1) % 10 == 0 and self.verbose:\n",
    "                print(f\"Epoch {i + 1}/{self.epochs}, Loss: {total_loss / x_train.shape[0]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ae9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(object):\n",
    "    def __init__(self, n_neighbors = 5):\n",
    "        self.k = n_neighbors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        dists = self.compute_distances(X)\n",
    "        pred = self.predict_labels(dists)\n",
    "        return pred\n",
    "\n",
    "    def compute_distances(self, X):\n",
    "        return np.linalg.norm(X[:, np.newaxis] - self.X_train, axis=2)\n",
    "\n",
    "    def predict_labels(self, dists):\n",
    "        num_test = dists.shape[0]\n",
    "        y_pred = np.zeros(num_test)\n",
    "\n",
    "        for i in range(num_test):\n",
    "            closest_y = []\n",
    "            sorted_dist = np.argsort(dists[i])\n",
    "            closest_y = list(self.y_train[sorted_dist[0:self.k]].ravel())\n",
    "            y_pred[i]= (np.argmax(np.bincount(closest_y)))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5206f51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 20:56:56.233066: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-24 20:56:56.234943: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-24 20:56:56.259661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-24 20:56:56.259684: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-24 20:56:56.260347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-24 20:56:56.264428: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-24 20:56:56.264856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-24 20:56:56.777815: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "(X_train,y_train),(X_test,y_test)=keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype('float64')\n",
    "X_test = X_test.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124a8e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (60000, 784))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "x_test = np.reshape(X_test, (10000, 28 * 28))\n",
    "x_train = np.reshape(X_train, (60000, 28 * 28))\n",
    "\n",
    "x_test, y_test = shuffle(x_test, y_test, random_state=10)\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=10)\n",
    "\n",
    "x_test.shape, x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3177eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23890/2789959103.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Loss: 0.07774568871373583\n",
      "Epoch 20/1000, Loss: 0.07352430292508211\n",
      "Epoch 30/1000, Loss: 0.07094143478236616\n",
      "Epoch 40/1000, Loss: 0.06978296545364758\n",
      "Epoch 50/1000, Loss: 0.0668134825032423\n",
      "Epoch 60/1000, Loss: 0.06274621627179058\n",
      "Epoch 70/1000, Loss: 0.06011913388872401\n",
      "Epoch 80/1000, Loss: 0.05855464591919672\n",
      "Epoch 90/1000, Loss: 0.05760344137263046\n",
      "Epoch 100/1000, Loss: 0.0562801031906433\n",
      "Epoch 110/1000, Loss: 0.05583082437879103\n",
      "Epoch 120/1000, Loss: 0.055382665206581395\n",
      "Epoch 130/1000, Loss: 0.05511589228560993\n",
      "Epoch 140/1000, Loss: 0.05479290623186562\n",
      "Epoch 150/1000, Loss: 0.05202711238953082\n",
      "Epoch 160/1000, Loss: 0.04959415383887853\n",
      "Epoch 170/1000, Loss: 0.05003690909832471\n",
      "Epoch 180/1000, Loss: 0.046264353615907895\n",
      "Epoch 190/1000, Loss: 0.0434069281752752\n",
      "Epoch 200/1000, Loss: 0.04262028004635758\n",
      "Epoch 210/1000, Loss: 0.04177167539196024\n",
      "Epoch 220/1000, Loss: 0.04093563582262053\n",
      "Epoch 230/1000, Loss: 0.0403306703198416\n",
      "Epoch 240/1000, Loss: 0.04075249108055204\n",
      "Epoch 250/1000, Loss: 0.04185857939237282\n",
      "Epoch 260/1000, Loss: 0.04098599624864882\n",
      "Epoch 270/1000, Loss: 0.04128000154770546\n",
      "Epoch 280/1000, Loss: 0.03848692754644754\n",
      "Epoch 290/1000, Loss: 0.03798235050576604\n",
      "Epoch 300/1000, Loss: 0.035975625979039715\n",
      "Epoch 310/1000, Loss: 0.03514305182986356\n",
      "Epoch 320/1000, Loss: 0.03601241301999093\n",
      "Epoch 330/1000, Loss: 0.03528077511998682\n",
      "Epoch 340/1000, Loss: 0.03487333651032316\n",
      "Epoch 350/1000, Loss: 0.035155759829873005\n",
      "Epoch 360/1000, Loss: 0.035242104391411774\n",
      "Epoch 370/1000, Loss: 0.0378576393523354\n",
      "Epoch 380/1000, Loss: 0.03644907048742496\n",
      "Epoch 390/1000, Loss: 0.03556085561580383\n",
      "Epoch 400/1000, Loss: 0.03501825877703471\n",
      "Epoch 410/1000, Loss: 0.03520380023804032\n",
      "Epoch 420/1000, Loss: 0.03442043742834786\n",
      "Epoch 430/1000, Loss: 0.0340036370451998\n",
      "Epoch 440/1000, Loss: 0.033731281372385014\n",
      "Epoch 450/1000, Loss: 0.03353179434849727\n",
      "Epoch 460/1000, Loss: 0.03334293312521151\n",
      "Epoch 470/1000, Loss: 0.03543826435655055\n",
      "Epoch 480/1000, Loss: 0.03416071989975935\n",
      "Epoch 490/1000, Loss: 0.033651810163534725\n",
      "Epoch 500/1000, Loss: 0.03328827737494956\n",
      "Epoch 510/1000, Loss: 0.03316474393680289\n",
      "Epoch 520/1000, Loss: 0.03289575675269114\n",
      "Epoch 530/1000, Loss: 0.034521318968857594\n",
      "Epoch 540/1000, Loss: 0.04181907470301923\n",
      "Epoch 550/1000, Loss: 0.03884757276308579\n",
      "Epoch 560/1000, Loss: 0.03753734520348608\n",
      "Epoch 570/1000, Loss: 0.03823439040258226\n",
      "Epoch 580/1000, Loss: 0.03792171896809705\n",
      "Epoch 590/1000, Loss: 0.037867687904541286\n",
      "Epoch 600/1000, Loss: 0.03692340750752875\n",
      "Epoch 610/1000, Loss: 0.03624751758988936\n",
      "Epoch 620/1000, Loss: 0.03578138759879718\n",
      "Epoch 630/1000, Loss: 0.03694572006797952\n",
      "Epoch 640/1000, Loss: 0.03631635924764443\n",
      "Epoch 650/1000, Loss: 0.035864450702819524\n",
      "Epoch 660/1000, Loss: 0.03560592488233375\n",
      "Epoch 670/1000, Loss: 0.03818557860790712\n",
      "Epoch 680/1000, Loss: 0.03987365341726555\n",
      "Epoch 690/1000, Loss: 0.038553448759258216\n",
      "Epoch 700/1000, Loss: 0.0378273414549752\n",
      "Epoch 710/1000, Loss: 0.0373037770395704\n",
      "Epoch 720/1000, Loss: 0.03693277787619489\n",
      "Epoch 730/1000, Loss: 0.04004061160017397\n",
      "Epoch 740/1000, Loss: 0.03783611632872887\n",
      "Epoch 750/1000, Loss: 0.03716757900854167\n",
      "Epoch 760/1000, Loss: 0.03671595185160581\n",
      "Epoch 770/1000, Loss: 0.03642190308263182\n",
      "Epoch 780/1000, Loss: 0.036215712388753214\n",
      "Epoch 790/1000, Loss: 0.03713755817622498\n",
      "Epoch 800/1000, Loss: 0.03672342806325496\n",
      "Epoch 810/1000, Loss: 0.036271671575986854\n",
      "Epoch 820/1000, Loss: 0.03600881140037138\n",
      "Epoch 830/1000, Loss: 0.03588392905935319\n",
      "Epoch 840/1000, Loss: 0.03573377717841385\n",
      "Epoch 850/1000, Loss: 0.03557650590842376\n",
      "Epoch 860/1000, Loss: 0.03543572463809774\n",
      "Epoch 870/1000, Loss: 0.03530156805528903\n",
      "Epoch 880/1000, Loss: 0.0351836337907947\n",
      "Epoch 890/1000, Loss: 0.03507844250855046\n",
      "Epoch 900/1000, Loss: 0.034982367772741454\n",
      "Epoch 910/1000, Loss: 0.034895972364898394\n",
      "Epoch 920/1000, Loss: 0.03481980990876185\n",
      "Epoch 930/1000, Loss: 0.03475281298811822\n",
      "Epoch 940/1000, Loss: 0.0346919657362332\n",
      "Epoch 950/1000, Loss: 0.036716141536573814\n",
      "Epoch 960/1000, Loss: 0.03635884701657566\n",
      "Epoch 970/1000, Loss: 0.03599828804528446\n",
      "Epoch 980/1000, Loss: 0.035786401640061076\n",
      "Epoch 990/1000, Loss: 0.03562996208238132\n",
      "Epoch 1000/1000, Loss: 0.035625391220559015\n",
      "NN accuracy: 0.0\n",
      "KNN accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "t_split = 1000\n",
    "p_split = 100\n",
    "\n",
    "Xtr = x_train[:t_split]\n",
    "Ytr = y_train[:t_split]\n",
    "\n",
    "Xte = x_test[:p_split]\n",
    "Yte = y_test[:p_split]\n",
    "\n",
    "model = KNN(3)\n",
    "model.fit(Xtr, Ytr)\n",
    "\n",
    "y_pred1 = model.predict(Xte)\n",
    "\n",
    "nn = NN(\n",
    "    [Layer(x_train[0].size, 200),\n",
    "     Sigmoid(),\n",
    "     Layer(200, 10),\n",
    "     Sigmoid(),\n",
    "    ],\n",
    "    epochs=1000,\n",
    "    learn_rate=0.1,\n",
    ")\n",
    "\n",
    "nn.train(Xtr, Ytr)\n",
    "y_pred2 = nn.predict(Xte)\n",
    "\n",
    "print(f\"NN accuracy: {accuracy_score(Yte, y_pred2)}\")\n",
    "print(f\"KNN accuracy: {accuracy_score(Yte, y_pred1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6f883",
   "metadata": {},
   "source": [
    "# Определение функций для аппроксимации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "905afe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "def func2(x):\n",
    "    return 5 * x**3 + x**2 + 5\n",
    "\n",
    "def func3(x):\n",
    "    return x * np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab278e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def error_uniform(x, func):\n",
    "    return func(x) + random.uniform(-0.5, 0.5)\n",
    "\n",
    "def gen(x, error_func, func):\n",
    "    return np.array([error_func(x_i, func) for x_i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38e9719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_graph(model, x, y, yp, x_o, y_o, func_name):\n",
    "    plt.title(f\"Name: {func_name}, Nodes: {model.layers[0].output_size}, layers: {len(model.layers) - 1}\")\n",
    "    plt.plot(x_o, y_o, 'b-')\n",
    "    plt.plot(x_o, yp, 'g-')\n",
    "    plt.plot(x, y, 'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d67ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models():\n",
    "    l = []\n",
    "    \n",
    "    for layers in [1, 2]:\n",
    "        for nodes in [1, 2, 3, 10]:\n",
    "            lay = [Layer(1, nodes), Sigmoid()]\n",
    "            for i in range(layers - 1):\n",
    "                lay.append(Layer(nodes, nodes), Sigmoid())\n",
    "            lay.append(Layer(nodes, 1), Sigmoid())\n",
    "            model = NN(lay, epochs=500, learn_rate=0.1, is_reg=True, verbose=False)\n",
    "            l.append(model)\n",
    "    return l\n",
    "\n",
    "def get_best_model(models, x_train, x_test, y_train, y_test):\n",
    "    best_model = models[0]\n",
    "    best_score = 100000000\n",
    "    for model in models:\n",
    "        model.train(x_train, y_train)\n",
    "        yp = model.predict(x_test)\n",
    "        score = np.mean((yp - y_test) ** 2)\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6965cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def do_all(error_func, func, func_name):\n",
    "    x_origin = np.linspace(-math.pi, math.pi, 200)\n",
    "    y_origin =  np.array([func(x_i) for x_i in x_origin])\n",
    "    \n",
    "    N = 100\n",
    "    x = np.sort(np.random.uniform(-math.pi, math.pi, N))\n",
    "    y = gen(x, error_uniform, func)\n",
    "    \n",
    "    models = create_models()\n",
    "    min_val = np.min(y)\n",
    "    max_val = np.max(y)\n",
    "    y_norm = (y - min_val) / (max_val - min_val)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y_norm, test_size=0.33, random_state=42)\n",
    "    model = get_best_model(models, x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    yp = model.predict(x_origin)\n",
    "    yp = yp * (max_val - min_val) + min_val\n",
    "    y_test = y_test * (max_val - min_val) + min_val\n",
    "    show_graph(model, x_test, y_test, yp, x_origin, y_origin, func_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a783b82",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdo_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_uniform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcos(x)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m do_all(error_uniform, func2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5 * x^3 + x^2 + 5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m do_all(error_uniform, func3, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx * sin(x)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m, in \u001b[0;36mdo_all\u001b[0;34m(error_func, func, func_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mpi, math\u001b[38;5;241m.\u001b[39mpi, N))\n\u001b[1;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m gen(x, error_uniform, func)\n\u001b[0;32m---> 11\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m min_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(y)\n\u001b[1;32m     13\u001b[0m max_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y)\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mcreate_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      8\u001b[0m     lay\u001b[38;5;241m.\u001b[39mappend(Layer(nodes, nodes), Sigmoid())\n\u001b[0;32m----> 9\u001b[0m \u001b[43mlay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m NN(lay, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, learn_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, is_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m l\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "do_all(error_uniform, func1, 'cos(x)')\n",
    "do_all(error_uniform, func2, '5 * x^3 + x^2 + 5')\n",
    "do_all(error_uniform, func3, 'x * sin(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62348750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428ad2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
